---
title: PINN
date: 2025-11-07 15:44:00
permalink: /pages/11947b/
categories:
  - 计算机
  - 大模型
tags:
  - 
---
# PINN

基于物理原理的神经网络，一种用于解决涉及非线性偏微分方程的正问题和逆问题的深度学习框架。

## 通用方程示例：

$$
u_t + \mathcal{N}[u; \lambda] = 0, \, x \in \Omega, \, t \in [0, T]
$$

$u(t, x)$：这是潜在的、未知的解函数

$u_t$：表示解函数 $u$ 对时间 $t$ 的偏导数。

$\mathcal{N}[u; λ]$：这是一个非线性微分算子，它作用于解函数 $u$，并且由一组参数 $λ$ 所参数化。

-  **“** 非线性 **”**  意味着算子 𝒩 中可能包含 $u$ 或其导数的高次项（如 $u²$, $uu_x$），这使得方程比线性方程复杂得多，也更难求解。
-  **“** 参数 $λ$​ **”**  是方程中的关键物理常数。例如，在流体力学中，它可能代表粘度；在热传导中，它可能代表热扩散率。

## **论文要解决的两大问题**

基于这个统一的PDE框架，论文明确提出了两个要解决的核心任务：

### **问题一：数据驱动的PDE求解（正问题）**

>  **“给定固定的模型参数** $λ$​ **，关于系统的未知隐藏状态** **$u(t, x)$** **我们能说什么？”**

	假设我们已经知道支配物理系统的精确方程（即我们知道算子 𝒩 的形式和参数 $λ$ 的值）。但是，我们无法直接获得整个时空域上的解$u(t, x)$。我们的目的是利用稀疏、嘈杂的观测数据，来推断（重建） 出整个时空域上完整的、连续的解 $u(t, x)$。好比我们知道牛顿定律（$F=ma$）和物体的质量，通过观测物体几个时刻的位置，来推断出它完整的运动轨迹。

### **问题二：数据驱动的PDE发现（反问题）**

>  **“哪些参数** **​`λ`​**​ **能最好地描述观测到的数据？”**

	假设我们观察到系统的大量数据，我们大致知道物理过程应该由哪类PDE描述（即我们知道算子$\mathcal{N}$$﻿$的形式），但我们不知道方程中关键的物理参数 $λ$ 是多少。目的就是从观测数据中学习、识别出这些未知的参数 $λ$。更进一步地说在某些研究中，甚至可以同时学习算子的形式和参数。就比如我们观察一个钟摆的摆动，我们假设它服从某种阻尼摆动规律，但不知道阻尼系数和自然频率是多少。通过摆动数据，我们来“发现”这些参数。

## 如何求解PDE？（问题1）

论文提出了两种算法模型解决这一问题。

### **连续时间模型**

	核心思想：不对方程进行任何离散化，而是直接用一个神经网络来逼近整个时空域上的未知解 $u(t, x)$，并通过将物理定律（PDE）作为约束来训练这个网络。

#### **1. 构建物理信息神经网络**

首先，论文从待求解的PDE出发：

$$
u_t + \mathcal{N}[u] = 0
$$

我们定义一个新的函数 $f(t, x)$，它由这个PDE的左端项给出：

$$
f:=u_t+\mathcal{N}[u]
$$

对于一个精确解 $u(t, x)$，$f(t, x)$ 应该处处为 **0**。

现在用一个深度神经网络 来近似解函数 $u(t, x)$ ⇒ $u_{NN}(t, x)$

由于 $f$ 是 $u$ 及其导数的组合，当我们用 $u_{NN}$ 替换 $u$时，我们就得到了另一个表达式 $f_{NN}(t, x)$。这个过程利用自动微分 来完成，它可以精确地计算神经网络输出对其输入（$t$​$,$​$x$）的导数。

**关键点：**

- 网络 $u_{NN}$ 和 $f_{NN}$共享相同的权重和偏置参数。
- $f_{NN}$ 不是一个独立的网络，而是由$u_{NN}$通过自动微分衍生而来的“物理信息”网络。
- 这就构成了一个物理信息神经网络，因为它内嵌了控制系统的物理定律。

---

#### **2. 设计损失函数**

如何训练这个共享参数的网络？通过定义一个特殊的损失函数来同时满足两个要求：**拟合数据** 和**遵守物理**。

损失函数由两部分组成：

$$
MSE=MSE_u+MSE_f
$$

**a) 数据损失**

$$
MSE_u = \frac{1}{N_u}\sum_{i=1}^{N_u} |u(t^i_u,x^i_u) - u^i|^2
$$

这一项描述了网络预测值 $u(t_u^i, x_u^i)$ 与真实测量值 $u^i$ 之间的均方误差。三个输入数据 {$t_u^i, x_u^i, u^i$}为初始条件和边界条件的观测数据。例如：

- $t=0$ 时的初始温度分布。
- 在整个时间范围内，边界 $x=0$ 和 $x=L$ 处的固定温度。

它能确保网络的预测结果贴合我们已知的少量真实数据。

**b) 物理损失**

$$
MSE_{f} = \frac{1}{N_{f}}\sum_{i=1}^{N_{f}} |f(t_{f}^{i}, x_{f}^{i})|^{2}
$$

这项描述了在域内一系列配点 上，强制PDE残差 $f$ 使其接近于0的损失。三个输入数据  **{**​**$t_f^i, x_f^i$**​ **}** ：这些点是在整个时空域 $Ω×[0,T]﻿$ 内随机或按策略（如拉丁超立方采样）选取的点，我们没有任何关于 $u$ 的真实测量值。

这一项充当了一个强大的物理正则化器。它告诉网络：“在这些没有数据的地方，你的预测也必须遵守PDE所描述的物理定律。”

### 离散时间模型

	在论文3.1节的末尾提到，连续时间模型需要在整个计算域内选取大量的配置点（$N_f$）来强制执行物理约束（即PDE本身）。在低维问题中尚可接受，但在高维问题中，配置点的数量会指数级增长，成为计算瓶颈。于是论文提出离散时间模型，它不再试图用一个神经网络去逼近整个时空的解 $u(t, x)$，而是将时间维度离散化，用神经网络去逼近在单个（或少数几个）大时间步长内的中间状态。该方法基于龙格库塔解微分方程的思路构建网络(并融入物理约束)和损失函数。

#### 1. 龙格库塔回顾

	对于一个一般的偏微分方程 $u_t + N[u] = 0$ ，应用q级龙格库塔方法，从一个时间步$t^n$ 到 $t^{n+1}$，可以得到如下公式：

$$
u^{n+c_i} = u^n - \Delta t \sum_{j=1}^q a_{ij} \mathcal{N}[u^{n+c_j}], \quad i = 1, \ldots, q
$$

$$
u^{n+1} = u^n - \Delta t \sum_{j=1}^q b_j \mathcal{N}[u^{n+c_j}]
$$

其中 $u^{n+c_i} = u(t_n + c_i \Delta t, x)$ 是第 $i$ 个中间阶段的解。

$c_i$是第 $i$ 个阶段的节点或相对时间位置。这些是介于 0 和 1 之间的常数，由具体的龙格-库塔法定义。它们决定了每个“中间阶段”在时间步 $[t^n, t^{n+1}]$ 内的位置。例如，对于中点法，$c = [0.5]$；对于经典的RK4，$c = [0, 0.5, 0.5, 1]$。

论文将上述方程巧妙地重新排列，得到一组定义在相同时间点上的等式：

$$
u^n(x) = u^{n+c_i}(x) + \Delta t \sum_{j=1}^q a_{ij} \mathcal{N}[u^{n+c_j}(x)] := u^n_i(x), \quad i = 1, \ldots, q
$$

$$
u^n(x) = u^{n+1}(x) + \Delta t \sum_{j=1}^q b_j \mathcal{N}[u^{n+c_j}(x)] := u^n_{q+1}(x)
$$

这个变换是关键。现在等式的左边都是初始状态  $u^n$，而右边是所有中间状态 $u^{n+c_j}$ 和最终状态 $u^{n+1}$ 的函数。接下来就可以根据上述两个公式构建网络和定义损失函数了。

#### 2. 构建物理信息神经网络

将一个q+1维输出网络的输出定义为所有中间状态和最终状态：

$$
[u^{n+c_1}(x), \ldots, u^{n+c_q}(x), u^{n+1}(x)]
$$

这个神经网络以空间坐标 $x$ 为输入。

**物理信息神经网络**：

将上面神经网络的输出代入变换后的龙格-库塔方程（即 $u^n_i(x)$ 和 $u^n_{q+1}(x)$ 的表达式），就自动得到了另一个网络，其输出为：

$$
[u_1^n(x), \ldots, u_q^n(x), u_{q+1}^n(x)]
$$

这个网络就是"物理信息"的载体，因为它内嵌了离散化的物理定律（PDE）。

#### 3. 设计损失函数

训练的目标是让物理信息神经网络的输出尽可能接近已知的初始数据 $u_n$。

损失函数为：

$$
SSE = SSE_n + SSE_b
$$

其中$SSE_n$是确保在初始时间步 $t_n$ 的采样点上，物理信息网络的输出 $u_i^n$ 与观测数据 $u_n$ 匹配。

$$
\begin{align*}
\mathrm{SSE}_n = \sum_{j=1}^{q+1} \sum_{i=1}^{N_n} \left| u^{n+c_j}(x_{n,i}) - u_{n,i} \right|^2
\end{align*}
$$

$SSE_b$是强制执行空间的边界条件。具体方程具体分析了。

## 如何发现PDE？（问题2）

	这节讲述如何用数据驱动发现偏微分方程，也提出了两种算法模型来解决这一问题，即连续时间模型和离散时间模型。

### 连续时间模型

	这一节的核心思想是：我们不仅用一个神经网络去拟合观测数据，还用另一个衍生网络去直接表示控制物理系统的PDE本身。通过将PDE的残差作为损失函数的一部分，我们同时学习系统的状态和其背后的物理定律。

#### 1.  构建物理信息神经网络

我们考虑一个参数化的非线性PDE，其一般形式为：

$$
u_t+\mathcal{N}[u;λ]=0, x∈Ω, t∈[0,T]
$$

- $u(t,x)$：系统的隐藏状态（我们只有它的部分噪声观测值）。

- $\mathcal{N}[u;λ]$：一个由参数 $λ$ 参数化的非线性微分算子。

我们的目的是从对 $u(t,x)$的噪声观测中，同时推断出完整的隐藏状态u(t,x) 和PDE的参数$λ$。

首先构建第一个网络来近似隐藏的状态$u(t,x)$，将其记为:

$$
u_{NN}(t,x,W,b)
$$

其中W，b是网络的权重和偏置。

> 一开始不太理解什么是用网络来近似隐藏的方程，查阅后明白：一个足够大的神经网络可以以任意精度逼近任何连续函数，实际上，这个神经网络本身就是一个函数，我们把它训练成我们想要的隐藏的$u(t,x)$

接着构建第二个网络，即用来约束物理规律的网络，它利用了第一个网络构造出$f(t,x)$

$$
f:=u_t+\mathcal{N}[u,λ]
$$

注意，这里的算子形式论文里假设是已知的。即虽然不知道方程具体啥样，但是知道它大概属于什么类型，我们的目的是求方程里的参数λ。

于是就有两个共享参数的神经网络：

- $u_{NN}(t,x)$：预测物理场。
- $f_{NN}(t,x;λ)$：强制执行物理定律

#### 2. 设计损失函数

	出发点是找到一组参数$(W,b,λ)$，使得$u_{NN}$和实际观测数据吻合，$f_{NN}$输出尽可能接近零。定义损失函数为：

$$
MSE=MSE_u+MSE_f
$$

其中

- $MSE_u = \frac{1}{N} \sum_{i=1}^{N} |u(t_u^i, x_u^i) - u^i|^2$
- $MSE_f = \frac{1}{N_f} \sum_{i=1}^{N_f} |f(t_f^i, x_f^i)|^2$

### 离散时间模型

	这里和3.2节的离散时间模型很像，就多了一个要学习的参数λ，网络结构大同小异，不作赘述了。

#### 1. 设计损失函数

这里先给出损失函数定义

$$
SSE = SSE_n + SSE_{n+1}
$$

为什么3.2节中的损失函数是$t_n$时刻的残差和边界限定，而4.2节中的是两个时刻的残差呢？

> 因为3.2节我们只有一个时间步的数据，但要预测整个时间演化。这是一个初值问题，需要边界条件来确保解的唯一性
>
> 在4.2节中，我们有两个时间步的完整数据。当知道系统在两个时间步的完整状态时，边界行为已经被数据唯一确定了。任何违反正确边界条件的解都会自动导致与$t_{n+1}$ 时刻数据的失配。

‍
